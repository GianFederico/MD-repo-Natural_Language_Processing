{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP58H/Q8Wgnslml6pOBihMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianFederico/MD-repo-Natural_Language_Processing/blob/main/NLP_lab3_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXd5dalhYjW3",
        "outputId": "a03feb17-4738-4fc8-901b-ec3e1c3d5a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#utils for cleaning process \n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('inaugural')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# split the input on anything other than a word character\n",
        "def onlywords(text):\n",
        "    cleaned_tokens = re.split(r'\\W+', text)\n",
        "    return cleaned_tokens\n",
        "\n",
        "# split on whitespace and then remove punct\n",
        "def wordsnopunct(text):\n",
        "    tokens=text.split()\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    return stripped\n",
        "\n",
        "def wordmatch(text):\n",
        "    cleaned_tokens = re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", text)\n",
        "    return cleaned_tokens\n",
        "\n",
        "def NLTKTokenize(text):\n",
        "    nltk_words = word_tokenize(text)\n",
        "    return nltk_words\n",
        "\n",
        "def NLTKregtokenize(text):\n",
        "    pattern = r'''(?x)\n",
        "    (?:[A-Z]\\.)+       \n",
        "   | \\w+(?:-\\w+)*       \n",
        "   | \\$?\\d+(?:\\.\\d+)?%? \n",
        "   | \\.\\.\\.             \n",
        "   | [][.,;\"'?():-_`]   \n",
        " '''\n",
        "    tokens=nltk.regexp_tokenize(text, pattern)\n",
        "    return tokens\n",
        "\n",
        "def onlypunct(text):\n",
        "    waste=re.findall(r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]+\", text)\n",
        "    return waste\n",
        "\n",
        "\n",
        "#TFIDF SCORE functions\n",
        "import math \n",
        "\n",
        "def create_df_table(dic,corpus_frequencies):\n",
        "    # create a df table as a dictionary (term, document frequency) \n",
        "    df_table = {}\n",
        "    for word in dic:\n",
        "        df_table[word]=0\n",
        "        for doc_id in corpus_frequencies.keys():\n",
        "            doc_freq=corpus_frequencies[doc_id]\n",
        "            if doc_freq[word]>0:\n",
        "                df_table[word] += 1\n",
        "    return df_table\n",
        "\n",
        "def create_idf_table(df_table, total_documents):\n",
        "    # create a df table as a dictionary (term, document frequency) \n",
        "    idf_table = {}\n",
        "    for word in df_table.keys():\n",
        "        idf_table[word]=math.log(total_documents / float(df_table[word]))\n",
        "    return idf_table\n",
        "\n",
        "def create_tfidf_table(f_distr, idf_table):\n",
        "    # create a tf*idf table (term, tf*idf score) from a doc frequency distribution\n",
        "    tf_idf_table = {}\n",
        "    for word in f_distr:\n",
        "        tf_idf_table[word]= float(f_distr.freq(word) * idf_table[word])\n",
        "    return tf_idf_table\n",
        "\n",
        "def compute_avg_tfidfscore(tf_idf_column):\n",
        "    # computing document relevance as avg. of tf*idf scores\n",
        "    doc_score=0\n",
        "    for word in tf_idf_column.keys():\n",
        "        doc_score+=tf_idf_column[word]\n",
        "    return doc_score / len(tf_idf_column)\n",
        "\n",
        "def compute_docvect_length(tf_idf_column):\n",
        "    # computing length of document vector\n",
        "    coordsum=0\n",
        "    for word in tf_idf_column.keys():\n",
        "        coordsum+=tf_idf_column[word]**2\n",
        "    return math.sqrt(coordsum)\n",
        "\n",
        "def compute_lenghtnorm_vectors(tf_idf_matrix):\n",
        "    # produces a length-normalized tf*idf matrix\n",
        "    norm_tf_idf_matrix = {}\n",
        "    for doc_id in tf_idf_matrix.keys():\n",
        "        column=tf_idf_matrix[doc_id]\n",
        "        vec_length=compute_docvect_length(column)\n",
        "        for word in column.keys():\n",
        "            column[word]=column[word]/vec_length\n",
        "        norm_tf_idf_matrix[doc_id]=column\n",
        "    return norm_tf_idf_matrix\n",
        "\n",
        "def compute_docweight_incorpus_table(scoring_table):\n",
        "    # computing document relevance in a corpus from a scoring table (doc, score) \n",
        "    all_weights=0\n",
        "    weight_table = {}\n",
        "    for doc_id in scoring_table.keys():\n",
        "        all_weights+=scoring_table[doc_id]\n",
        "    for doc_id in scoring_table.keys():\n",
        "        weight_table[doc_id]=(scoring_table[doc_id] / all_weights)\n",
        "        # normalized scores\n",
        "    return weight_table\n",
        "\n",
        "def compute_cos_similarity(norm_vec1, norm_vec2):\n",
        "    # computing cosine similarity between 2 normalized vectors\n",
        "    common_words=[w for w in norm_vec1 if w in norm_vec2]\n",
        "    sim_score=0\n",
        "    for w in common_words:\n",
        "        sim_score+=norm_vec1[w]*norm_vec2[w]\n",
        "    return sim_score\n",
        "\n",
        "def compute_centroid(voc, tf_idf_matrix):\n",
        "    # produces the centroid of vectors in a tf*idf matrix\n",
        "    centroid = {}\n",
        "    length=len(tf_idf_matrix.keys())\n",
        "    for word in voc:\n",
        "        centroid[word]=0\n",
        "    for doc_id in tf_idf_matrix.keys():\n",
        "        column=tf_idf_matrix[doc_id]\n",
        "        for word in column.keys():\n",
        "            centroid[word]+=column[word]/length\n",
        "    return centroid\n",
        "\n",
        "def normalize_vector(vector):\n",
        "    # produces a length-normalized vector\n",
        "    norm_vect = {}\n",
        "    vec_length=compute_docvect_length(vector)\n",
        "    for word in vector.keys():\n",
        "        norm_vect[word]=vector[word]/vec_length\n",
        "    return norm_vect\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ATTEMPT 1\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import inaugural\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "# load text\n",
        "#text=inaugural.raw(\"1817-Monroe.txt\")\n",
        "text=inaugural.raw(\"2013-Obama.txt\")\n",
        "#text=inaugural.raw(\"2017-Trump.txt\")\n",
        "sentences = sent_tokenize(text) # NLTK function\n",
        "corpus={}\n",
        "# our corpus is the set of sentences in the text\n",
        "# one doc = one sentence\n",
        "\n",
        "for sent in sentences:\n",
        "        corpus[sent]=sent\n",
        "\n",
        "# apply the pipeline on each doc to build a frequency table\n",
        "\n",
        "\n",
        "total_documents = len(corpus)\n",
        "stop_words = stopwords.words('english')\n",
        "frequencies = {}\n",
        "vocabulary=set()\n",
        "vocabularysize=0\n",
        "for doc_id in corpus.keys():\n",
        "    tokens= wordmatch(corpus[doc_id])\n",
        "    waste = onlypunct(corpus[doc_id])\n",
        "    cleaned_tokens = [t for t in tokens if not t in waste]\n",
        "    nostop_tokens = [t for t in cleaned_tokens if not t in stop_words]\n",
        "    norm_tokens = [t.lower() for t in nostop_tokens]\n",
        "    #porter = PorterStemmer()\n",
        "    #stemmed_tokens = [porter.stem(t) for t in norm_tokens]\n",
        "    wnl=nltk.WordNetLemmatizer()\n",
        "    lemmas = [wnl.lemmatize(t) for t in norm_tokens]\n",
        "    docvocabulary=set(lemmas)\n",
        "    vocabulary=vocabulary.union(docvocabulary)\n",
        "    vocabularysize=vocabularysize+len(docvocabulary)\n",
        "    docfdist = nltk.FreqDist(t for t in lemmas)      \n",
        "    frequencies[doc_id]=docfdist\n",
        "# building a df table (term, #sentences that contain term)\n",
        "df_table=create_df_table(vocabulary, frequencies)\n",
        "# building an idf table (term, idf score) based on a df_table\n",
        "idf_table=create_idf_table(df_table, total_documents)\n",
        "#show.printdict(idf_table)\n",
        "\n",
        "# Now we build a tf*idf_table for each sentence\n",
        "# building a tf*idf table from: 1) a frequency distribution 2) an idf_table\n",
        "#               doc1    doc2    ...     docN\n",
        "# term1         w11     w12     ...     w1N\n",
        "# term2         w21     w22     ...     w2N\n",
        "# ...           ...     ...     ...     ...\n",
        "# termM         wM1     wM2     ...     wMN\n",
        "\n",
        "# A tf_idf_table corresponds to a column of the classic term-doc matrix\n",
        "# We build the whole matrix indexed by docs\n",
        "\n",
        "tf_idf_tables={}\n",
        "for doc_id in frequencies.keys():\n",
        "      tf_idf_table=create_tfidf_table(frequencies[doc_id], idf_table)\n",
        "      tf_idf_tables[doc_id]=tf_idf_table\n",
        "\n",
        "# Now we need to assign a relevance to a sentence\n",
        "\n",
        "# computing a table containing a score for each doc in the corpus (from each tfidf_table)\n",
        "tfidf_sentence_score_table={}\n",
        "for sent in tf_idf_tables.keys():\n",
        "        tfidf_sentence_score_table[sent]=compute_avg_tfidfscore(tf_idf_tables[sent])\n",
        "\n",
        "\n",
        "# computing a table containing a relevance score for each sentence (from a scoring table)\n",
        "relevance_table=compute_docweight_incorpus_table(tfidf_sentence_score_table)\n",
        "\n",
        "# generating a summary based on some criteria on relevance table\n",
        "# selection criterion --> relevance score of sentence > avg. relevance \n",
        "def generate_summary(relevance_table):\n",
        "    summary=\"\"\n",
        "    n_sentences=len(relevance_table.keys())\n",
        "    sentence_count = 0\n",
        "    total_rel=0\n",
        "    for sentence in relevance_table.keys():\n",
        "        total_rel+=relevance_table[sentence]\n",
        "    avgrel=total_rel/n_sentences\n",
        "    for sentence in relevance_table.keys():\n",
        "        if relevance_table[sentence]>=avgrel:\n",
        "            #print (\"taken!\", relevance_table[sentence], avgrel)\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count+=1\n",
        "    #used=int((sentence_count / n_sentences)*100)    \n",
        "    summary += \"\\n Used: \"+str(sentence_count)+\" out of \"+str(n_sentences)+\" sentences.\"\n",
        "    return summary\n",
        "\n",
        "summary=generate_summary(relevance_table)\n",
        "\n",
        "#sentence_relevance_matrix=tfidf.sentence_relevance(tfidfscoring)\n",
        "#print (sentence_relevance_matrix)\n",
        "#threshold=tfidf.find_average_score(sentence_relevance_matrix)\n",
        "#print (\"Avg. relevance score per sentence: \", threshold)\n",
        "#print (\"All sentences above the threshold will be included in the summary\")\n",
        "#summary=tfidf.generate_summary(sentences, sentence_relevance_matrix, threshold)\n",
        "print (\"SUMMARY\\n\")\n",
        "print (summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLEkTqTFaESp",
        "outputId": "195fb25d-257b-41c1-c2f7-e54253e41767"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY\n",
            "\n",
            " Thank you. Thank you so much. We affirm the promise of our democracy. And for more than 200 years, we have. We made ourselves anew, and vowed to move forward together. This generation of Americans has been tested by crises that steeled our resolve and proved our resilience. A decade of war is now ending. An economic recovery has begun. We understand that outworn programs are inadequate to the needs of our time. That is what this moment requires. That is what will give real meaning to our creed. We do not believe that in this country freedom is reserved for the lucky, or happiness for the few. The path towards sustainable energy sources will be long and sometimes difficult. But America cannot resist this transition, we must lead it. That is how we will preserve our planet, commanded to our care by God. That's what will lend meaning to the creed our fathers once declared. America will remain the anchor of strong alliances in every corner of the globe. It is now our generation's task to carry on what those pioneers began. For now decisions are upon us and we cannot afford delay. We must act, knowing that our work will be imperfect. And we must faithfully execute that pledge during the duration of our service. They are the words of citizens and they represent our greatest hope. You and I, as citizens, have the power to set this country's course. God bless you, and may He forever bless these United States of America.\n",
            " Used: 24 out of 89 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ATTEMPT 2\n",
        "\n",
        "# load text\n",
        "text=inaugural.raw(\"2013-Obama.txt\")\n",
        "#text=inaugural.raw(\"2017-Trump.txt\")\n",
        "sentences = sent_tokenize(text) # NLTK function\n",
        "corpus={}\n",
        "# our corpus is the set of sentences in the text\n",
        "# one doc = one sentence\n",
        "\n",
        "for sent in sentences:\n",
        "        corpus[sent]=sent\n",
        "\n",
        "# apply the pipeline on each doc to build a frequency table\n",
        "\n",
        "\n",
        "total_documents = len(corpus)\n",
        "stop_words = stopwords.words('english')\n",
        "frequencies = {}\n",
        "vocabulary=set()\n",
        "vocabularysize=0\n",
        "for doc_id in corpus.keys():\n",
        "        tokens=wordmatch(corpus[doc_id])\n",
        "        waste = onlypunct(corpus[doc_id])\n",
        "        cleaned_tokens = [t for t in tokens if not t in waste]\n",
        "        nostop_tokens = [t for t in cleaned_tokens if not t in stop_words]\n",
        "        norm_tokens = [t.lower() for t in nostop_tokens]\n",
        "        #porter = PorterStemmer()\n",
        "        #stemmed_tokens = [porter.stem(t) for t in norm_tokens]\n",
        "        wnl=nltk.WordNetLemmatizer()\n",
        "        lemmas = [wnl.lemmatize(t) for t in norm_tokens]\n",
        "        docvocabulary=set(lemmas)\n",
        "        vocabulary=vocabulary.union(docvocabulary)\n",
        "        vocabularysize=vocabularysize+len(docvocabulary)\n",
        "        docfdist = nltk.FreqDist(t for t in lemmas)      \n",
        "        frequencies[doc_id]=docfdist\n",
        "# building a df table (term, #sentences that contain term)\n",
        "df_table=create_df_table(vocabulary, frequencies)\n",
        "# building an idf table (term, idf score) based on a df_table\n",
        "idf_table=create_idf_table(df_table, total_documents)\n",
        "#show.printdict(idf_table)\n",
        "\n",
        "# Now we build a tf*idf_table for each sentence\n",
        "# building a tf*idf table from: 1) a frequency distribution 2) an idf_table\n",
        "#               doc1    doc2    ...     docN\n",
        "# term1         w11     w12     ...     w1N\n",
        "# term2         w21     w22     ...     w2N\n",
        "# ...           ...     ...     ...     ...\n",
        "# termM         wM1     wM2     ...     wMN\n",
        "\n",
        "# A tf_idf_table corresponds to a column of the classic term-doc matrix\n",
        "# We build the whole matrix indexed by docs\n",
        "\n",
        "tf_idf_tables={}\n",
        "for doc_id in frequencies.keys():\n",
        "        tf_idf_table=create_tfidf_table(frequencies[doc_id], idf_table)\n",
        "        tf_idf_tables[doc_id]=tf_idf_table\n",
        "\n",
        "###### ANOTHER SOLUTION BASED ON NORMALIZED VECTORS #######\n",
        "\n",
        "\n",
        "#### we want to length-normalize the tf-idf vectors ###\n",
        "tf_idf_tables=compute_lenghtnorm_vectors(tf_idf_tables)\n",
        "\n",
        "# Now we need to assign a relevance to a sentence\n",
        "\n",
        "# computing a table containing a score for each doc in the corpus (from each tfidf_table)\n",
        "tfidf_sentence_score_table={}\n",
        "for sent in tf_idf_tables.keys():\n",
        "        tfidf_sentence_score_table[sent]=compute_avg_tfidfscore(tf_idf_tables[sent])\n",
        "\n",
        "\n",
        "# computing a table containing a relevance score for each sentence (from a scoring table)\n",
        "relevance_table=compute_docweight_incorpus_table(tfidf_sentence_score_table)\n",
        "#show.printdict(relevance_table)\n",
        "# generating a summary based on some criteria on relevance table\n",
        "\n",
        "# selection criterion --> relevance score of sentence > avg. relevance \n",
        "def generate_summary(relevance_table):\n",
        "    summary=\"\"\n",
        "    n_sentences=len(relevance_table.keys())\n",
        "    sentence_count = 0\n",
        "    total_rel=0\n",
        "    for sentence in relevance_table.keys():\n",
        "        total_rel+=relevance_table[sentence]\n",
        "    avgrel=total_rel/n_sentences\n",
        "    for sentence in relevance_table.keys():\n",
        "        if relevance_table[sentence]>=avgrel:\n",
        "            #print (\"taken!\", relevance_table[sentence], avgrel)\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count+=1\n",
        "    #used=int((sentence_count / n_sentences)*100)    \n",
        "    summary += \"\\n Used: \"+str(sentence_count)+\" out of \"+str(n_sentences)+\" sentences.\"\n",
        "    return summary\n",
        "\n",
        "\n",
        "summary=generate_summary(relevance_table)\n",
        "\n",
        "#sentence_relevance_matrix=tfidf.sentence_relevance(tfidfscoring)\n",
        "#print (sentence_relevance_matrix)\n",
        "#threshold=tfidf.find_average_score(sentence_relevance_matrix)\n",
        "#print (\"Avg. relevance score per sentence: \", threshold)\n",
        "#print (\"All sentences above the threshold will be included in the summary\")\n",
        "#summary=tfidf.generate_summary(sentences, sentence_relevance_matrix, threshold)\n",
        "print (\"SUMMARY\\n\")\n",
        "print (summary)\n",
        "\n",
        "#######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z94KP1_jcPUi",
        "outputId": "50c3ab36-770f-430e-e614-660962e37199"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY\n",
            "\n",
            " Thank you. Thank you so much. We affirm the promise of our democracy. Today we continue a never-ending journey to bridge the meaning of those words with the realities of our time. The patriots of 1776 did not fight to replace the tyranny of a king with the privileges of a few or the rule of a mob. And for more than 200 years, we have. We made ourselves anew, and vowed to move forward together. Together, we discovered that a free market only thrives when there are rules to ensure competition and fair play. Now more than ever, we must do these things together, as one nation and one people. This generation of Americans has been tested by crises that steeled our resolve and proved our resilience. A decade of war is now ending. An economic recovery has begun. We understand that outworn programs are inadequate to the needs of our time. That is what this moment requires. That is what will give real meaning to our creed. We do not believe that in this country freedom is reserved for the lucky, or happiness for the few. We, the people, still believe that our obligations as Americans are not just to ourselves, but to all posterity. The path towards sustainable energy sources will be long and sometimes difficult. But America cannot resist this transition, we must lead it. That is how we will preserve our planet, commanded to our care by God. That's what will lend meaning to the creed our fathers once declared. The knowledge of their sacrifice will keep us forever vigilant against those who would do us harm. We will defend our people and uphold our values through strength of arms and rule of law. America will remain the anchor of strong alliances in every corner of the globe. It is now our generation's task to carry on what those pioneers began. It does not mean we all define liberty in exactly the same way or follow the same precise path to happiness. For now decisions are upon us and we cannot afford delay. We must act, knowing that our work will be imperfect. And we must faithfully execute that pledge during the duration of our service. My oath is not so different from the pledge we all make to the flag that waves above and that fills our hearts with pride. They are the words of citizens and they represent our greatest hope. You and I, as citizens, have the power to set this country's course. Let us, each of us, now embrace with solemn duty and awesome joy what is our lasting birthright. God bless you, and may He forever bless these United States of America.\n",
            " Used: 34 out of 89 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ATTEMPT 3\n",
        "\n",
        "import operator\n",
        "# load text\n",
        "text=inaugural.raw(\"2013-Obama.txt\")\n",
        "#text=inaugural.raw(\"2017-Trump.txt\")\n",
        "sentences = sent_tokenize(text) # NLTK function\n",
        "corpus={}\n",
        "# our corpus is the set of sentences in the text\n",
        "# one doc = one sentence\n",
        "\n",
        "for sent in sentences:\n",
        "        corpus[sent]=sent\n",
        "\n",
        "# apply the pipeline on each doc to build a frequency table\n",
        "\n",
        "\n",
        "total_documents = len(corpus)\n",
        "stop_words = stopwords.words('english')\n",
        "frequencies = {}\n",
        "vocabulary=set()\n",
        "vocabularysize=0\n",
        "for doc_id in corpus.keys():\n",
        "        tokens=wordmatch(corpus[doc_id])\n",
        "        waste =onlypunct(corpus[doc_id])\n",
        "        cleaned_tokens = [t for t in tokens if not t in waste]\n",
        "        nostop_tokens = [t for t in cleaned_tokens if not t in stop_words]\n",
        "        norm_tokens = [t.lower() for t in nostop_tokens]\n",
        "        #porter = PorterStemmer()\n",
        "        #stemmed_tokens = [porter.stem(t) for t in norm_tokens]\n",
        "        wnl=nltk.WordNetLemmatizer()\n",
        "        lemmas = [wnl.lemmatize(t) for t in norm_tokens]\n",
        "        docvocabulary=set(lemmas)\n",
        "        vocabulary=vocabulary.union(docvocabulary)\n",
        "        vocabularysize=vocabularysize+len(docvocabulary)\n",
        "        docfdist = nltk.FreqDist(t for t in lemmas)      \n",
        "        frequencies[doc_id]=docfdist\n",
        "# building a df table (term, #sentences that contain term)\n",
        "df_table=create_df_table(vocabulary, frequencies)\n",
        "# building an idf table (term, idf score) based on a df_table\n",
        "idf_table=create_idf_table(df_table, total_documents)\n",
        "#show.printdict(idf_table)\n",
        "\n",
        "# Now we build a tf*idf_table for each sentence\n",
        "# building a tf*idf table from: 1) a frequency distribution 2) an idf_table\n",
        "#               doc1    doc2    ...     docN\n",
        "# term1         w11     w12     ...     w1N\n",
        "# term2         w21     w22     ...     w2N\n",
        "# ...           ...     ...     ...     ...\n",
        "# termM         wM1     wM2     ...     wMN\n",
        "\n",
        "# A tf_idf_table corresponds to a column of the classic term-doc matrix\n",
        "# We build the whole matrix indexed by docs\n",
        "\n",
        "tf_idf_tables={}\n",
        "for doc_id in frequencies.keys():\n",
        "        tf_idf_table=create_tfidf_table(frequencies[doc_id], idf_table)\n",
        "        tf_idf_tables[doc_id]=tf_idf_table\n",
        "\n",
        "###### ANOTHER SOLUTION BASED ON NORMALIZED VECTORS AND k-NN#######\n",
        "\n",
        "\n",
        "#### we want to length-normalize the tf-idf vectors ###\n",
        "tf_idf_tables=compute_lenghtnorm_vectors(tf_idf_tables)\n",
        "\n",
        "# Instead of assigning a relevance to a sentence we:\n",
        "# compute the centroid vector of the document and find most similar sentences\n",
        "\n",
        "\n",
        "# computing the centroid from a vocabulary and a tf*idf matrix\n",
        "centroid=compute_centroid(vocabulary, tf_idf_tables)\n",
        "norm_centroid=normalize_vector(centroid)\n",
        "#print (\"centroid length:\", tfidf.compute_docvect_length(centroid))\n",
        "#print (\"centroid normalized length:\", tfidf.compute_docvect_length(norm_centroid))\n",
        "# generating a summary by selecting k most similar sentences to the centroid  \n",
        "\n",
        "k=34\n",
        "\n",
        "\n",
        "def generate_summary(tf_idf_tables, norm_centroid, k):\n",
        "    summary=\"\"\n",
        "    sentence_list={}\n",
        "    for sentence in tf_idf_tables.keys():\n",
        "        sim=compute_cos_similarity(centroid,tf_idf_tables[sentence])\n",
        "        sentence_list[sentence]=sim\n",
        "    # rank sentence_list\n",
        "    sorted_sim_table = dict( sorted(sentence_list.items(), key=operator.itemgetter(1),reverse=True))\n",
        "    sorted_topk = dict(list(sorted_sim_table.items())[0: k])\n",
        "    for sentence in sorted_topk:\n",
        "        summary += \" \" + sentence\n",
        "    # select top-k sentences\n",
        "    return summary\n",
        "\n",
        "summary=generate_summary(tf_idf_tables, norm_centroid, k)\n",
        "\n",
        "#sentence_relevance_matrix=tfidf.sentence_relevance(tfidfscoring)\n",
        "#print (sentence_relevance_matrix)\n",
        "#threshold=tfidf.find_average_score(sentence_relevance_matrix)\n",
        "#print (\"Avg. relevance score per sentence: \", threshold)\n",
        "#print (\"All sentences above the threshold will be included in the summary\")\n",
        "#summary=tfidf.generate_summary(sentences, sentence_relevance_matrix, threshold)\n",
        "\n",
        "print (\"SUMMARY made of: \", str(k), \"sentences\\n\")\n",
        "print (summary)\n",
        "\n",
        "#######################################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7R-U6WzfCNi",
        "outputId": "09fde82e-c2cc-4594-e40c-1029618d79b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY made of:  34 sentences\n",
            "\n",
            " That is our generation's taskâto make these words, these rights, these values of life and liberty and the pursuit of happiness real for every American. They do not make us a nation of takers; they free us to take the risks that make this country great. We, the people, still believe that enduring security and lasting peace do not require perpetual war. We, the people, still believe that our obligations as Americans are not just to ourselves, but to all posterity. We, the people, still believe that every citizen deserves a basic measure of security and dignity. What makes us exceptionalâwhat makes us Americanâis our allegiance to an idea articulated in a declaration made more than two centuries ago:\n",
            "\n",
            "We hold these truths to be self-evident, that all men are created equal; that they are endowed by their Creator with certain unalienable rights; that among these are life, liberty, and the pursuit of happiness. Progress does not compel us to settle centuries-long debates about the role of government for all time, but it does require us to act in our time. They gave to us a republic, a government of and by and for the people, entrusting each generation to keep safe our founding creed. Now more than ever, we must do these things together, as one nation and one people. We cannot cede to other nations the technology that will power new jobs and new industries, we must claim its promise. For history tells us that while these truths may be self-evident, they've never been self-executing; that while freedom is a gift from God, it must be secured by His people here on Earth. But we have always understood that when times change, so must we; that fidelity to our founding principles requires new responses to new challenges; that preserving our individual freedoms ultimately requires collective action. We, the people, declare today that the most evident of truthsâthat all of us are created equalâis the star that guides us still; just as it guided our forebears through Seneca Falls and Selma and Stonewall; just as it guided all those men and women, sung and unsung, who left footprints along this great Mall, to hear a preacher say that we cannot walk alone; to hear a King proclaim that our individual freedom is inextricably bound to the freedom of every soul on Earth. And we must be a source of hope to the poor, the sick, the marginalized, the victims of prejudiceânot out of mere charity, but because peace in our time requires the constant advance of those principles that our common creed describes: tolerance and opportunity, human dignity and justice. For we, the people, understand that our country cannot succeed when a shrinking few do very well and a growing many barely make it. We must act, knowing that our work will be imperfect. But we are also heirs to those who won the peace and not just the war; who turned sworn enemies into the surest of friendsâand we must carry those lessons into this time as well. We do not believe that in this country freedom is reserved for the lucky, or happiness for the few. My fellow Americans, the oath I have sworn before you today, like the one recited by others who serve in this Capitol, was an oath to God and country, not party or faction. We will support democracy from Asia to Africa, from the Americas to the Middle East, because our interests and our conscience compel us to act on behalf of those who long for freedom. We must act, we must act knowing that today's victories will be only partial and that it will be up to those who stand here in 4 years and 40 years and 400 years hence to advance the timeless spirit once conferred to us in a spare Philadelphia hall. But while the means will change, our purpose endures: a nation that rewards the effort and determination of every single American. We are true to our creed when a little girl born into the bleakest poverty knows that she has the same chance to succeed as anybody else, because she is an American; she is free and she is equal, not just in the eyes of God, but also in our own. Our journey is not complete until our gay brothers and sisters are treated like anyone else under the lawâfor if we are truly created equal, then surely the love we commit to one another must be equal as well. But we reject the belief that America must choose between caring for the generation that built this country and investing in the generation that will build its future. But the words I spoke today are not so different from the oath that is taken each time a soldier signs up for duty or an immigrant realizes her dream. My fellow Americans, we are made for this moment and we will seize itâso long as we seize it together. Together, we resolved that a great nation must care for the vulnerable and protect its people from life's worst hazards and misfortune. You and I, as citizens, have the obligation to shape the debates of our timeânot only with the votes we cast, but with the voices we lift in defense of our most ancient values and enduring ideals. The knowledge of their sacrifice will keep us forever vigilant against those who would do us harm. We recognize that no matter how responsibly we live our lives, any one of us at any time may face a job loss or a sudden illness or a home swept away in a terrible storm. But America cannot resist this transition, we must lead it. Our journey is not complete until we find a better way to welcome the striving, hopeful immigrants who still see America as a land of opportunityâuntil bright young students and engineers are enlisted in our workforce rather than expelled from our country. That is what will give real meaning to our creed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlRUtpw5frn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}