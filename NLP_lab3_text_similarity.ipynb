{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN09vm4YE2U1nOX4qQPsf9b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianFederico/MD-repo-Natural_Language_Processing/blob/main/NLP_lab3_text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_WyHQlGik-D",
        "outputId": "27b4f6e8-5bf5-4db4-9f68-0e30c3888881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# In general, the similarity between two items is measured according to their features.\n",
        "# For text documents, we need to define:\n",
        "# – A document representation strategy, i.e. features that describe the texts\n",
        "# – A similarity (or distance) function based on features\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import inaugural\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from itertools import groupby, combinations\n",
        "import operator\n",
        "nltk.download('inaugural')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def wordmatch(text):\n",
        "    cleaned_tokens = re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", text)\n",
        "    return cleaned_tokens\n",
        "\n",
        "def onlypunct(text):\n",
        "    waste=re.findall(r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]+\", text)\n",
        "    return waste\n",
        "\n",
        "# def NLTKTokenize(text):\n",
        "#     nltk_words = word_tokenize(text)\n",
        "#     return nltk_words\n",
        "\n",
        "# def NLTKregtokenize(text):\n",
        "#     pattern = r'''(?x)\n",
        "#     (?:[A-Z]\\.)+\n",
        "#    | \\w+(?:-\\w+)*\n",
        "#    | \\$?\\d+(?:\\.\\d+)?%?\n",
        "#    | \\.\\.\\.\n",
        "#    | [][.,;\"'?():-_`]\n",
        "#  '''\n",
        "#     tokens=nltk.regexp_tokenize(text, pattern)\n",
        "#     return tokens\n",
        "\n",
        "# # split the input on anything other than a word character\n",
        "# def onlywords(text):\n",
        "#     cleaned_tokens = re.split(r'\\W+', text)\n",
        "#     return cleaned_tokens\n",
        "\n",
        "# # split on whitespace and then remove punct\n",
        "# def wordsnopunct(text):\n",
        "#     tokens=text.split()\n",
        "#     table = str.maketrans('', '', string.punctuation)\n",
        "#     stripped = [w.translate(table) for w in tokens]\n",
        "#     return stripped\n",
        "\n",
        "\n",
        "#TFIDF SCORE functions\n",
        "def create_df_table(dic,corpus_frequencies):\n",
        "    # create a df table as a dictionary (term, document frequency)\n",
        "    df_table = {}\n",
        "    for word in dic:\n",
        "        df_table[word]=0\n",
        "        for doc_id in corpus_frequencies.keys():\n",
        "            doc_freq=corpus_frequencies[doc_id]\n",
        "            if doc_freq[word]>0:\n",
        "                df_table[word] += 1\n",
        "    return df_table\n",
        "\n",
        "def create_idf_table(df_table, total_documents):\n",
        "    # create a df table as a dictionary (term, document frequency)\n",
        "    idf_table = {}\n",
        "    for word in df_table.keys():\n",
        "        idf_table[word]=math.log(total_documents / float(df_table[word]))\n",
        "    return idf_table\n",
        "\n",
        "def create_tfidf_table(f_distr, idf_table):\n",
        "    # create a tf*idf table (term, tf*idf score) from a doc frequency distribution\n",
        "    tf_idf_table = {}\n",
        "    for word in f_distr:\n",
        "        tf_idf_table[word]= float(f_distr.freq(word) * idf_table[word])\n",
        "    return tf_idf_table\n",
        "\n",
        "def compute_docvect_length(tf_idf_column):\n",
        "    # computing length of document vector\n",
        "    coordsum=0\n",
        "    for word in tf_idf_column.keys():\n",
        "        coordsum+=tf_idf_column[word]**2\n",
        "    return math.sqrt(coordsum)\n",
        "\n",
        "def compute_lenghtnorm_vectors(tf_idf_matrix):\n",
        "    # produces a length-normalized tf*idf matrix\n",
        "    norm_tf_idf_matrix = {}\n",
        "    for doc_id in tf_idf_matrix.keys():\n",
        "        column=tf_idf_matrix[doc_id]\n",
        "        vec_length=compute_docvect_length(column)\n",
        "        for word in column.keys():\n",
        "            column[word]=column[word]/vec_length\n",
        "        norm_tf_idf_matrix[doc_id]=column\n",
        "    return norm_tf_idf_matrix\n",
        "\n",
        "def compute_cos_similarity(norm_vec1, norm_vec2):\n",
        "    # computing cosine similarity between 2 normalized vectors\n",
        "    common_words=[w for w in norm_vec1 if w in norm_vec2]\n",
        "    sim_score=0\n",
        "    for w in common_words:\n",
        "        sim_score+=norm_vec1[w]*norm_vec2[w]\n",
        "    return sim_score\n",
        "\n",
        "# def compute_centroid(voc, tf_idf_matrix):\n",
        "#     # produces the centroid of vectors in a tf*idf matrix\n",
        "#     centroid = {}\n",
        "#     length=len(tf_idf_matrix.keys())\n",
        "#     for word in voc:\n",
        "#         centroid[word]=0\n",
        "#     for doc_id in tf_idf_matrix.keys():\n",
        "#         column=tf_idf_matrix[doc_id]\n",
        "#         for word in column.keys():\n",
        "#             centroid[word]+=column[word]/length\n",
        "#     return centroid\n",
        "\n",
        "# def normalize_vector(vector):\n",
        "#     # produces a length-normalized vector\n",
        "#     norm_vect = {}\n",
        "#     vec_length=compute_docvect_length(vector)\n",
        "#     for word in vector.keys():\n",
        "#         norm_vect[word]=vector[word]/vec_length\n",
        "#     return norm_vect\n",
        "\n",
        "# def compute_avg_tfidfscore(tf_idf_column):\n",
        "#     # computing document relevance as avg. of tf*idf scores\n",
        "#     doc_score=0\n",
        "#     for word in tf_idf_column.keys():\n",
        "#         doc_score+=tf_idf_column[word]\n",
        "#     return doc_score / len(tf_idf_column)\n",
        "\n",
        "# def compute_docweight_incorpus_table(scoring_table):\n",
        "#     # computing document relevance in a corpus from a scoring table (doc, score)\n",
        "#     all_weights=0\n",
        "#     weight_table = {}\n",
        "#     for doc_id in scoring_table.keys():\n",
        "#         all_weights+=scoring_table[doc_id]\n",
        "#     for doc_id in scoring_table.keys():\n",
        "#         weight_table[doc_id]=(scoring_table[doc_id] / all_weights)\n",
        "#         # normalized scores\n",
        "#     return weight_table\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JACCARD SIMILARITY\n",
        "\n",
        "corpus={}\n",
        "for doc_id in nltk.corpus.inaugural.fileids():\n",
        "        corpus[doc_id]=inaugural.raw(doc_id)\n",
        "\n",
        "# apply the pipeline on each doc to build a frequency table\n",
        "total_documents = len(corpus)\n",
        "stop_words = stopwords.words('english')\n",
        "frequencies = {}\n",
        "vocabulary=set()\n",
        "vocabularysize=0\n",
        "for doc_id in corpus.keys():\n",
        "        tokens=wordmatch(corpus[doc_id])\n",
        "        waste = onlypunct(corpus[doc_id])\n",
        "        cleaned_tokens = [t for t in tokens if not t in waste]\n",
        "        nostop_tokens = [t for t in cleaned_tokens if not t in stop_words]\n",
        "        norm_tokens = [t.lower() for t in nostop_tokens]\n",
        "        #porter = PorterStemmer()\n",
        "        #stemmed_tokens = [porter.stem(t) for t in norm_tokens]\n",
        "        wnl=nltk.WordNetLemmatizer()\n",
        "        lemmas = [wnl.lemmatize(t) for t in norm_tokens]\n",
        "        docvocabulary=set(lemmas)\n",
        "        vocabulary=vocabulary.union(docvocabulary)\n",
        "        vocabularysize=vocabularysize+len(docvocabulary)\n",
        "        docfdist = nltk.FreqDist(t for t in lemmas)      \n",
        "        frequencies[doc_id]=docfdist\n",
        "# building a df table (term, #sentences that contain term)\n",
        "df_table=create_df_table(vocabulary, frequencies)\n",
        "# building an idf table (term, idf score) based on a df_table\n",
        "idf_table=create_idf_table(df_table, total_documents)\n",
        "#show.printdict(idf_table)\n",
        "\n",
        "# Now we build a tf*idf_table for each sentence\n",
        "# building a tf*idf table from: 1) a frequency distribution 2) an idf_table\n",
        "#               doc1    doc2    ...     docN\n",
        "# term1         w11     w12     ...     w1N\n",
        "# term2         w21     w22     ...     w2N\n",
        "# ...           ...     ...     ...     ...\n",
        "# termM         wM1     wM2     ...     wMN\n",
        "\n",
        "# A tf_idf_table corresponds to a column of the classic term-doc matrix\n",
        "# We build the whole matrix indexed by docs\n",
        "\n",
        "tf_idf_tables={}\n",
        "for doc_id in frequencies.keys():\n",
        "        tf_idf_table=create_tfidf_table(frequencies[doc_id], idf_table)\n",
        "        tf_idf_tables[doc_id]=tf_idf_table\n",
        "\n",
        "#### we want to length-normalize the tf-idf vectors ###\n",
        "tf_idf_tables=compute_lenghtnorm_vectors(tf_idf_tables)\n",
        "\n",
        "def compute_Jacc_sim(v1,v2):\n",
        "        # compute Jaccard similarity between two lists of terms\n",
        "        set1=set(v1)\n",
        "        set2=set(v2)\n",
        "        union = set1 | set2\n",
        "        intersection = set([w for w in set1 if w in set2])\n",
        "        return len(intersection)/len(union)\n",
        "\n",
        "pairs=combinations(tf_idf_tables.keys(), 2)\n",
        "all_pairs=list(pairs)\n",
        "sim_table={}\n",
        "for w in all_pairs:\n",
        "        key=\"\"\n",
        "        words1=tf_idf_tables[w[0]].keys()\n",
        "        words2=tf_idf_tables[w[1]].keys()\n",
        "        sim=compute_Jacc_sim(words1,words2)\n",
        "        key=w[0][:len(w[0])-4]+\"---\"+w[1][:len(w[1])-4]\n",
        "        sim_table[key]=sim\n",
        "\n",
        "sorted_sim_table = dict(sorted(sim_table.items(), key=operator.itemgetter(1),reverse=True))\n",
        "# take only top-k similarities\n",
        "print (\"TOP-K JACCARD SIMILARITIES:\")\n",
        "K=3\n",
        "sorted_topk = dict(list(sorted_sim_table.items())[0: K])\n",
        "#show.printdict(sorted_topk)\n",
        "print(sorted_topk)\n",
        "sorted_lastk = dict(list(sorted_sim_table.items())[len(sorted_sim_table)-K:])\n",
        "print (\"LAST-K JACCARD SIMILARITIES:\")\n",
        "#show.printdict(sorted_lastk)\n",
        "print(sorted_lastk)\n",
        "print(\"____________________________\")\n",
        "print (\"SPECIFIC COMPARISONS:\")\n",
        "key=\"2009-Obama---2013-Obama\"\n",
        "print (key,\" \", sim_table[key])\n",
        "key=\"2013-Obama---2017-Trump\"\n",
        "print (key, \" \", sim_table[key])\n",
        "key=\"2005-Bush---2013-Obama\"\n",
        "print (key, \" \", sim_table[key])\n",
        "key=\"2005-Bush---2017-Trump\"\n",
        "print (key, \" \", sim_table[key])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrGWKYqFkdF5",
        "outputId": "e65b7a91-232f-4f16-b7a1-4f33ddea9af1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-K JACCARD SIMILARITIES:\n",
            "{'1817-Monroe---1821-Monroe': 0.2804476629361422, '1837-VanBuren---1841-Harrison': 0.25321100917431194, '1837-VanBuren---1845-Polk': 0.25124515771997785}\n",
            "LAST-K JACCARD SIMILARITIES:\n",
            "{'1793-Washington---2009-Obama': 0.026570048309178744, '1793-Washington---1841-Harrison': 0.025153374233128835, '1793-Washington---1997-Clinton': 0.021961932650073207}\n",
            "____________________________\n",
            "SPECIFIC COMPARISONS:\n",
            "2009-Obama---2013-Obama   0.22798353909465022\n",
            "2013-Obama---2017-Trump   0.18674698795180722\n",
            "2005-Bush---2013-Obama   0.21005385996409337\n",
            "2005-Bush---2017-Trump   0.1696149843912591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#COSINE SIMILARITY\n",
        "\n",
        "corpus={}\n",
        "for doc_id in nltk.corpus.inaugural.fileids():\n",
        "        corpus[doc_id]=inaugural.raw(doc_id)\n",
        "\n",
        "# apply the pipeline on each doc to build a frequency table\n",
        "total_documents = len(corpus)\n",
        "stop_words = stopwords.words('english')\n",
        "frequencies = {}\n",
        "vocabulary=set()\n",
        "vocabularysize=0\n",
        "for doc_id in corpus.keys():\n",
        "        tokens=wordmatch(corpus[doc_id])\n",
        "        waste = onlypunct(corpus[doc_id])\n",
        "        cleaned_tokens = [t for t in tokens if not t in waste]\n",
        "        nostop_tokens = [t for t in cleaned_tokens if not t in stop_words]\n",
        "        norm_tokens = [t.lower() for t in nostop_tokens]\n",
        "        #porter = PorterStemmer()\n",
        "        #stemmed_tokens = [porter.stem(t) for t in norm_tokens]\n",
        "        wnl=nltk.WordNetLemmatizer()\n",
        "        lemmas = [wnl.lemmatize(t) for t in norm_tokens]\n",
        "        docvocabulary=set(lemmas)\n",
        "        vocabulary=vocabulary.union(docvocabulary)\n",
        "        vocabularysize=vocabularysize+len(docvocabulary)\n",
        "        docfdist = nltk.FreqDist(t for t in lemmas)      \n",
        "        frequencies[doc_id]=docfdist\n",
        "# building a df table (term, #sentences that contain term)\n",
        "df_table=create_df_table(vocabulary, frequencies)\n",
        "# building an idf table (term, idf score) based on a df_table\n",
        "idf_table=create_idf_table(df_table, total_documents)\n",
        "#show.printdict(idf_table)\n",
        "\n",
        "# Now we build a tf*idf_table for each sentence\n",
        "# building a tf*idf table from: 1) a frequency distribution 2) an idf_table\n",
        "#               doc1    doc2    ...     docN\n",
        "# term1         w11     w12     ...     w1N\n",
        "# term2         w21     w22     ...     w2N\n",
        "# ...           ...     ...     ...     ...\n",
        "# termM         wM1     wM2     ...     wMN\n",
        "\n",
        "# A tf_idf_table corresponds to a column of the classic term-doc matrix\n",
        "# We build the whole matrix indexed by docs\n",
        "\n",
        "tf_idf_tables={}\n",
        "for doc_id in frequencies.keys():\n",
        "        tf_idf_table=create_tfidf_table(frequencies[doc_id], idf_table)\n",
        "        tf_idf_tables[doc_id]=tf_idf_table\n",
        "\n",
        "#### we want to length-normalize the tf-idf vectors ###\n",
        "tf_idf_tables=compute_lenghtnorm_vectors(tf_idf_tables)\n",
        "\n",
        "pairs=combinations(tf_idf_tables.keys(), 2)\n",
        "all_pairs=list(pairs)\n",
        "sim_table={}\n",
        "for w in all_pairs:\n",
        "        key=\"\"\n",
        "        # take the two vectors\n",
        "        words1=tf_idf_tables[w[0]]\n",
        "        words2=tf_idf_tables[w[1]]\n",
        "        sim=compute_cos_similarity(words1,words2)\n",
        "        key=w[0][:len(w[0])-4]+\"---\"+w[1][:len(w[1])-4]\n",
        "        sim_table[key]=sim\n",
        "\n",
        "sorted_sim_table = dict( sorted(sim_table.items(), key=operator.itemgetter(1),reverse=True))\n",
        "# take only top-k similarities\n",
        "print (\"TOP-K COSINE SIMILARITIES:\")\n",
        "K=3\n",
        "sorted_topk = dict(list(sorted_sim_table.items())[0: K])\n",
        "#show.printdict(sorted_topk)\n",
        "print(sorted_topk)\n",
        "sorted_lastk = dict(list(sorted_sim_table.items())[len(sorted_sim_table)-K:])\n",
        "print (\"LAST-K COSINE SIMILARITIES:\")\n",
        "#show.printdict(sorted_lastk)\n",
        "print(sorted_lastk)\n",
        "print(\"____________________________\")\n",
        "print (\"SPECIFIC COMPARISONS:\")\n",
        "key=\"2009-Obama---2013-Obama\"\n",
        "print (key, \" \", sim_table[key])\n",
        "key=\"2013-Obama---2017-Trump\"\n",
        "print (key, \" \", sim_table[key])\n",
        "key=\"2005-Bush---2013-Obama\" \n",
        "print (key, \" \", sim_table[key])\n",
        "key=\"2005-Bush---2017-Trump\"\n",
        "print (key, \" \", sim_table[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxyUu-svqLVT",
        "outputId": "82e7325e-6325-4469-a34d-64c3ad7ce24b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-K COSINE SIMILARITIES:\n",
            "{'2013-Obama---2021-Biden': 0.3351574327645749, '1817-Monroe---1821-Monroe': 0.28641925416692, '1837-VanBuren---1841-Harrison': 0.24784710485922665}\n",
            "LAST-K COSINE SIMILARITIES:\n",
            "{'1793-Washington---1913-Wilson': 0.00845784472040958, '1793-Washington---1865-Lincoln': 0.008201545078799308, '1793-Washington---1905-Roosevelt': 0.0041607134804818304}\n",
            "____________________________\n",
            "SPECIFIC COMPARISONS:\n",
            "2009-Obama---2013-Obama   0.18878927738334836\n",
            "2013-Obama---2017-Trump   0.18118560541687198\n",
            "2005-Bush---2013-Obama   0.08195075702143417\n",
            "2005-Bush---2017-Trump   0.08221561078669792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSEaNVcCqbi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}